{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(in_file, max_example=None, relabeling=True):\n",
    "    \"\"\"\n",
    "        load CNN / Daily Mail data from {train | dev | test}.txt\n",
    "        relabeling: relabel the entities by their first occurence if it is True.\n",
    "    \"\"\"\n",
    "\n",
    "    documents = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            question = line.strip().lower()\n",
    "            answer = f.readline().strip()\n",
    "            document = f.readline().strip().lower()\n",
    "\n",
    "            if relabeling:\n",
    "                q_words = question.split(' ')\n",
    "                d_words = document.split(' ')\n",
    "                assert answer in d_words\n",
    "\n",
    "                entity_dict = {}\n",
    "                entity_id = 0\n",
    "                for word in d_words + q_words:\n",
    "                    if (word.startswith('@entity')) and (word not in entity_dict):\n",
    "                        entity_dict[word] = '@entity' + str(entity_id)\n",
    "                        entity_id += 1\n",
    "\n",
    "                q_words = [entity_dict[w] if w in entity_dict else w for w in q_words]\n",
    "                d_words = [entity_dict[w] if w in entity_dict else w for w in d_words]\n",
    "                answer = entity_dict[answer]\n",
    "\n",
    "                question = ' '.join(q_words)\n",
    "                document = ' '.join(d_words)\n",
    "\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "            documents.append(document)\n",
    "            num_examples += 1\n",
    "\n",
    "            f.readline()\n",
    "            if (max_example is not None) and (num_examples >= max_example):\n",
    "                break\n",
    "                \n",
    "    print('#Examples: %d' % len(documents))\n",
    "    return (documents, questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Train Loading\n",
      "#Examples: 100\n",
      "********** Dev Loading\n",
      "#Examples: 100\n"
     ]
    }
   ],
   "source": [
    "fin_train = 'data/cnn/train.txt'\n",
    "fin_dev = 'data/cnn/dev.txt'\n",
    "\n",
    "print('*' * 10 + ' Train Loading')\n",
    "train_d, train_q, train_a = load_data(fin_train, 100, relabeling=True)\n",
    "print('*' * 10 + ' Dev Loading')\n",
    "dev_d, dev_q, dev_a = load_data(fin_dev, 100, relabeling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days after two @entity0 journalists were killed in northern @entity1 , authorities rounded up dozens of suspects and a group linked to @entity2 claimed responsibility for the deaths . at least 30 suspects were seized in desert camps near the town of @entity3 and taken to the local @entity0 army base for questioning , three officials in @entity1 said . the officials did not want to be named because they are not authorized to talk to the media . @entity4 ( @entity4 ) has allegedly claimed responsibility for the killings , according to @entity5 news agency in @entity6 . @entity4 operates in northern @entity7 and the group 's statements have shown up before on the @entity8 outlet . @entity9 journalists @entity10 and @entity11 were abducted in front of the home of a member of the @entity12 rebels ' @entity13 of a @entity14 on saturday , @entity9 reported . they were found dead the same day . their bodies arrived in @entity15 on tuesday . @entity3 was one of the strongholds of the @entity16 militant @entity12 uprising last year that plunged @entity1 into chaos after a military - led coup . following the coup , @entity12 rebels occupied the northern half of the country . a response to \" crimes \" against @entity16 in @entity17 @entity4 said the killings were in response to the \" crimes \" perpetrated by @entity0 as well as @entity7 and international troops against @entity16 in @entity17 . @entity17 is an area in northern @entity1 that separatist @entity12 rebels describe as the cradle of their nomadic civilization . @entity4 said that this is just the beginning and that @entity0 president @entity18 will pay more in response to this \" new crusade \" against @entity16 , according to the purported claim . veteran @entity19 war correspondent kidnapped in @entity20 as part of @entity0 's intervention this year to flush out militants in @entity1 , the @entity0 military secured the area around @entity3 . @entity18 called an emergency meeting with ministers sunday after the killings .\n",
      "officials : the suspects were taken to the local @placeholder army base for questioning\n",
      "@entity0\n"
     ]
    }
   ],
   "source": [
    "print(train_d[0])\n",
    "print(train_q[0])\n",
    "print(train_a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(sentences, max_words=50000):\n",
    "    \"\"\"\n",
    "        Build a dictionary for the words in `sentences`.\n",
    "        Only the max_words ones are kept and the remaining will be mapped to <UNK>.\n",
    "    \"\"\"\n",
    "    word_count = Counter()\n",
    "    for sent in sentences:\n",
    "        for w in sent.split(' '):\n",
    "            word_count[w] += 1\n",
    "\n",
    "    ls = word_count.most_common(max_words)\n",
    "    print('#Words: %d -> %d' % (len(word_count), len(ls)))\n",
    "    for key in ls[:5]:\n",
    "        print(key)\n",
    "    print('...')\n",
    "    for key in ls[-5:]:\n",
    "        print(key)\n",
    "\n",
    "    # leave 0 to UNK\n",
    "    # leave 1 to delimiter |||\n",
    "    return {w[0]: index + 2 for (index, w) in enumerate(ls)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build dictionary..\n",
      "#Words: 8178 -> 8178\n",
      "('the', 4399)\n",
      "(',', 4044)\n",
      "('.', 3356)\n",
      "('\"', 2181)\n",
      "('to', 1992)\n",
      "...\n",
      "('spout', 1)\n",
      "('tide', 1)\n",
      "('envoy', 1)\n",
      "('heroic', 1)\n",
      "('quirkier', 1)\n",
      "Entity markers: 225\n"
     ]
    }
   ],
   "source": [
    "print('Build dictionary..')\n",
    "word_dict = build_dict(train_d + train_q)\n",
    "entity_markers = list(set([w for w in word_dict.keys()\n",
    "                          if w.startswith('@entity')] + train_a))\n",
    "entity_markers = ['<unk_entity>'] + entity_markers\n",
    "entity_dict = {w: index for (index, w) in enumerate(entity_markers)}\n",
    "print('Entity markers: %d' % len(entity_dict))\n",
    "num_labels = len(entity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
