{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(in_file, max_example=None, relabeling=True):\n",
    "    \"\"\"\n",
    "        load CNN / Daily Mail data from {train | dev | test}.txt\n",
    "        relabeling: relabel the entities by their first occurence if it is True.\n",
    "    \"\"\"\n",
    "\n",
    "    documents = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            question = line.strip().lower()\n",
    "            answer = f.readline().strip()\n",
    "            document = f.readline().strip().lower()\n",
    "\n",
    "            if relabeling:\n",
    "                q_words = question.split(' ')\n",
    "                d_words = document.split(' ')\n",
    "                assert answer in d_words\n",
    "\n",
    "                entity_dict = {}\n",
    "                entity_id = 0\n",
    "                for word in d_words + q_words:\n",
    "                    if (word.startswith('@entity')) and (word not in entity_dict):\n",
    "                        entity_dict[word] = '@entity' + str(entity_id)\n",
    "                        entity_id += 1\n",
    "\n",
    "                q_words = [entity_dict[w] if w in entity_dict else w for w in q_words]\n",
    "                d_words = [entity_dict[w] if w in entity_dict else w for w in d_words]\n",
    "                answer = entity_dict[answer]\n",
    "\n",
    "                question = ' '.join(q_words)\n",
    "                document = ' '.join(d_words)\n",
    "\n",
    "            questions.append(question)\n",
    "            answers.append(answer)\n",
    "            documents.append(document)\n",
    "            num_examples += 1\n",
    "\n",
    "            f.readline()\n",
    "            if (max_example is not None) and (num_examples >= max_example):\n",
    "                break\n",
    "                \n",
    "    print('#Examples: %d' % len(documents))\n",
    "    return (documents, questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Train Loading\n",
      "#Examples: 100\n",
      "********** Dev Loading\n",
      "#Examples: 100\n"
     ]
    }
   ],
   "source": [
    "fin_train = 'data/cnn/train.txt'\n",
    "fin_dev = 'data/cnn/dev.txt'\n",
    "\n",
    "print('*' * 10 + ' Train Loading')\n",
    "train_d, train_q, train_a = load_data(fin_train, 100, relabeling=True)\n",
    "print('*' * 10 + ' Dev Loading')\n",
    "dev_d, dev_q, dev_a = load_data(fin_dev, 100, relabeling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days after two @entity0 journalists were killed in northern @entity1 , authorities rounded up dozens of suspects and a group linked to @entity2 claimed responsibility for the deaths . at least 30 suspects were seized in desert camps near the town of @entity3 and taken to the local @entity0 army base for questioning , three officials in @entity1 said . the officials did not want to be named because they are not authorized to talk to the media . @entity4 ( @entity4 ) has allegedly claimed responsibility for the killings , according to @entity5 news agency in @entity6 . @entity4 operates in northern @entity7 and the group 's statements have shown up before on the @entity8 outlet . @entity9 journalists @entity10 and @entity11 were abducted in front of the home of a member of the @entity12 rebels ' @entity13 of a @entity14 on saturday , @entity9 reported . they were found dead the same day . their bodies arrived in @entity15 on tuesday . @entity3 was one of the strongholds of the @entity16 militant @entity12 uprising last year that plunged @entity1 into chaos after a military - led coup . following the coup , @entity12 rebels occupied the northern half of the country . a response to \" crimes \" against @entity16 in @entity17 @entity4 said the killings were in response to the \" crimes \" perpetrated by @entity0 as well as @entity7 and international troops against @entity16 in @entity17 . @entity17 is an area in northern @entity1 that separatist @entity12 rebels describe as the cradle of their nomadic civilization . @entity4 said that this is just the beginning and that @entity0 president @entity18 will pay more in response to this \" new crusade \" against @entity16 , according to the purported claim . veteran @entity19 war correspondent kidnapped in @entity20 as part of @entity0 's intervention this year to flush out militants in @entity1 , the @entity0 military secured the area around @entity3 . @entity18 called an emergency meeting with ministers sunday after the killings .\n",
      "officials : the suspects were taken to the local @placeholder army base for questioning\n",
      "@entity0\n"
     ]
    }
   ],
   "source": [
    "print(train_d[0])\n",
    "print(train_q[0])\n",
    "print(train_a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dict(sentences, max_words=50000):\n",
    "    \"\"\"\n",
    "        Build a dictionary for the words in `sentences`.\n",
    "        Only the max_words ones are kept and the remaining will be mapped to <UNK>.\n",
    "    \"\"\"\n",
    "    word_count = Counter()\n",
    "    for sent in sentences:\n",
    "        for w in sent.split(' '):\n",
    "            word_count[w] += 1\n",
    "\n",
    "    ls = word_count.most_common(max_words)\n",
    "    print('#Words: %d -> %d' % (len(word_count), len(ls)))\n",
    "    for key in ls[:5]:\n",
    "        print(key)\n",
    "    print('...')\n",
    "    for key in ls[-5:]:\n",
    "        print(key)\n",
    "\n",
    "    # leave 0 to UNK\n",
    "    # leave 1 to delimiter |||\n",
    "    return {w[0]: index + 2 for (index, w) in enumerate(ls)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build dictionary..\n",
      "#Words: 8178 -> 8178\n",
      "('the', 4399)\n",
      "(',', 4044)\n",
      "('.', 3356)\n",
      "('\"', 2181)\n",
      "('to', 1992)\n",
      "...\n",
      "('spout', 1)\n",
      "('tide', 1)\n",
      "('envoy', 1)\n",
      "('heroic', 1)\n",
      "('quirkier', 1)\n",
      "Entity markers: 225\n"
     ]
    }
   ],
   "source": [
    "print('Build dictionary..')\n",
    "word_dict = build_dict(train_d + train_q)\n",
    "entity_markers = list(set([w for w in word_dict.keys()\n",
    "                          if w.startswith('@entity')] + train_a))\n",
    "entity_markers = ['<unk_entity>'] + entity_markers\n",
    "entity_dict = {w: index for (index, w) in enumerate(entity_markers)}\n",
    "print('Entity markers: %d' % len(entity_dict))\n",
    "num_labels = len(entity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_embeddings(word_dict, dim, in_file=None):\n",
    "    \"\"\"\n",
    "        Generate an initial embedding matrix for `word_dict`.\n",
    "        If an embedding file is not given or a word is not in the embedding file,\n",
    "        a randomly initialized vector will be used.\n",
    "    \"\"\"\n",
    "\n",
    "    num_words = len(word_dict) + 2\n",
    "    embeddings = np.random.uniform(size=(num_words, dim))\n",
    "    print('Embeddings: %d x %d' % (num_words, dim))\n",
    "\n",
    "    if in_file is not None:\n",
    "        print('Loading embedding file: %s' % in_file)\n",
    "        pre_trained = 0\n",
    "        for line in open(in_file).readlines():\n",
    "            sp = line.split()\n",
    "            assert len(sp) == dim + 1 # word + embeddings ..\n",
    "            if sp[0] in word_dict:\n",
    "                pre_trained += 1\n",
    "                embeddings[word_dict[sp[0]]] = [float(x) for x in sp[1:]]\n",
    "        print('Pre-trained: %d (%.2f%%)' %\n",
    "              (pre_trained, pre_trained * 100.0 / num_words))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 8180 x 50\n",
      "Loading embedding file: data/glove.6B/glove.6B.50d.txt\n",
      "Pre-trained: 7887 (96.42%)\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 50\n",
    "embeddings = gen_embeddings(word_dict, embedding_size, 'data/glove.6B/glove.6B.{}d.txt'.format(embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, word_dict, embeddings, embedding_dim, hidden_dim):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.word_dict = word_dict\n",
    "        self.embeddings = embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim // 2 * 2\n",
    "\n",
    "        self.d_gru = nn.GRU(embedding_dim, self.hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "        self.q_gru = nn.GRU(embedding_dim, self.hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Variable(num_layers*num_directions, minibatch_size, hidden_dim)\n",
    "        return Variable(torch.randn(2, 1, self.hidden_dim // 2))\n",
    "    \n",
    "    def forward(self, d, q):\n",
    "        d_words = d.split()\n",
    "        q_words = q.split()\n",
    "        d_idx = [self.word_dict[dw] for dw in d_words]\n",
    "        q_idx = [self.word_dict[qw] for qw in q_words]\n",
    "        d_emb = [self.embeddings[i] for i in d_idx] # !bug: max_words not in word_dict\n",
    "        q_emb = [self.embeddings[i] for i in q_idx]\n",
    "        d_emb = Variable(torch.FloatTensor(d_emb), requires_grad=True)\n",
    "        q_emb = Variable(torch.FloatTensor(q_emb), requires_grad=True)\n",
    "        \n",
    "        d_hidden = self.init_hidden()\n",
    "        q_hidden = self.init_hidden()\n",
    "        d_gru_out, d_hidden = self.d_gru(d_emb.view(len(d_words), 1, -1), # (seq_len, batch, input_size)\n",
    "                                         d_hidden)\n",
    "        q_gru_out, q_hidden = self.q_gru(q_emb.view(len(q_words), 1, -1), # (seq_len, batch, input_size)\n",
    "                                         q_hidden)\n",
    "        \n",
    "        d_gru_out = d_gru_out.view(len(d_words), self.hidden_dim)\n",
    "        q_gru_out = q_gru_out.view(len(q_words), self.hidden_dim)\n",
    "        sim = torch.mm(d_gru_out, q_hidden.view(self.hidden_dim,1))\n",
    "        prob = F.softmax(sim, dim=0)\n",
    "        \n",
    "        prob_uniq = defaultdict(float)\n",
    "        for i,p in zip(q_idx, prob):\n",
    "            prob_uniq[i] += p\n",
    "        return prob_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 2208\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  3.0187\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "idx: 2\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.2678\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "idx: 452\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.5723\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "idx: 6\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  3.4829\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "idx: 1512\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.9110\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "idx: 1596\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  1.9943\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "idx: 491\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  1.9940\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "idx: 2946\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  2.6689\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "idx: 14\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.5144\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "idx: 104\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  1.1711\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "idx: 59\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  7.4605\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "idx: 188\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  1.5242\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "idx: 30\n",
      "Variable containing:\n",
      "1.00000e-03 *\n",
      "  1.6161\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 5\n",
    "net = Net(word_dict, embeddings, embedding_size, hidden_dim)\n",
    "for idx, d,q,a in zip(range(len(train_d)), train_d, train_q, train_a): # !test\n",
    "    prob = net(d, q)\n",
    "    for k,v in prob.items():\n",
    "        print('idx: {}'.format(k))\n",
    "        print(v)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
